{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Classification-CNN-for-NLP-v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPxpembVP50ZwwkE3cWvL52",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smf-9000/Text-Intent-Classification/blob/main/Text_Classification_CNN_for_NLP_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uCLvsLl0eiq"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbvHJvHEj4C2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import text, sequence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTPT3s7CXQHk",
        "outputId": "714e4014-f8f5-451e-9392-3349d0355dad"
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 15:39:40--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  28.7MB/s    in 2.8s    \n",
            "\n",
            "2021-07-08 15:39:43 (28.7 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg5d_JVtXQEo"
      },
      "source": [
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yABbUJ9adjM"
      },
      "source": [
        "num_words = 30000\n",
        "seq_max_len = 500"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChTzkuyGZ_dG"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            labels.append(0 if label_dir is \"neg\" else 1)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test')\n",
        "\n",
        "train_labels = np.asarray(train_labels)\n",
        "test_labels = np.asarray(test_labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvW5BEacZ_aD"
      },
      "source": [
        "tokenizer = text.Tokenizer(num_words)\n",
        "tokenizer.fit_on_texts(np.concatenate([train_texts, test_texts]))\n",
        "\n",
        "train_tokenized = tokenizer.texts_to_sequences(train_texts) \n",
        "train_inputs = sequence.pad_sequences(train_tokenized, maxlen=seq_max_len)\n",
        "\n",
        "test_tokenized = tokenizer.texts_to_sequences(test_texts) \n",
        "test_inputs = sequence.pad_sequences(test_tokenized, maxlen=seq_max_len)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G85Hk1XpHY7G",
        "outputId": "a0116ecd-6211-409a-d77e-529351d29dba"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 15:54:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-07-08 15:54:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-07-08 15:54:23--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.16MB/s    in 2m 41s  \n",
            "\n",
            "2021-07-08 15:57:04 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9gki707HkEX"
      },
      "source": [
        "glove_dim = 200\n",
        "glove_file = '/content/glove.6B.' + str(glove_dim) + 'd.txt'\n",
        "emb_dict = {}\n",
        "glove = open(glove_file)\n",
        "for line in glove:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  emb_dict[word] = vector\n",
        "glove.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nauA8KESHy5Z"
      },
      "source": [
        "emb_matrix = np.zeros((num_words, glove_dim))\n",
        "for w, i in tokenizer.word_index.items():\n",
        "  if i < num_words:\n",
        "    vect = emb_dict.get(w)\n",
        "    if vect is not None:\n",
        "      emb_matrix[i] = vect\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53Uqxr-hZ_Xh"
      },
      "source": [
        ""
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbu7EGrzocFP"
      },
      "source": [
        "## dcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OjS9pkhBH1"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            emb_dim=128,\n",
        "            nb_filters=50,\n",
        "            FFN_units=512,\n",
        "            nb_classes=2,\n",
        "            dropout_rate=0.1,\n",
        "            training=False,\n",
        "            name=\"dcnn\",\n",
        "            ngrams=[2,3,4]):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(\n",
        "                            vocab_size,\n",
        "                            emb_dim,\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(emb_matrix),\n",
        "                            trainable=False)\n",
        "        \n",
        "        self.conv1d_list = []\n",
        "        for n in ngrams:\n",
        "          conv_tmp = layers.Conv1D(\n",
        "                            filters=nb_filters,\n",
        "                            kernel_size=n,\n",
        "                            padding=\"valid\",\n",
        "                            activation=\"relu\")\n",
        "          self.conv1d_list.append(conv_tmp)\n",
        "\n",
        "        self.conv1 = layers.Conv1D(\n",
        "                        filters=nb_filters,\n",
        "                        kernel_size=3,\n",
        "                        padding=\"valid\",\n",
        "                        activation=\"relu\")\n",
        "        \n",
        "        self.conv2 = layers.Conv1D(\n",
        "                        filters=nb_filters,\n",
        "                        kernel_size=5,\n",
        "                        padding=\"valid\",\n",
        "                        activation=\"relu\")\n",
        "        \n",
        "        self.pool_1 = layers.MaxPooling1D()\n",
        "        self.pool_2 = layers.GlobalMaxPool1D()\n",
        "\n",
        "        self.dense_1 = layers.Dense(\n",
        "                            units=FFN_units,\n",
        "                            activation=\"relu\")\n",
        "        \n",
        "        self.dropout_e = layers.Dropout(rate=dropout_rate)\n",
        "        self.dropout_d = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=1,\n",
        "                            activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=nb_classes,\n",
        "                            activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout_e(x, training)\n",
        "\n",
        "        x_1 = self.conv1(x)\n",
        "        x_1 = self.pool_1(x_1)\n",
        "        x_1 = self.conv2(x_1)\n",
        "        x_1 = self.pool_2(x_1)\n",
        "\n",
        "        x_2 = [x_1]\n",
        "        for i, _ in enumerate(self.conv1d_list):\n",
        "          x_t = self.conv1d_list[i](x)\n",
        "          x_t = self.pool_2(x_t)\n",
        "          x_2.append(x_t)\n",
        "\n",
        "        output = tf.concat(x_2, axis=-1)\n",
        "        output = self.dense_1(output)\n",
        "        output = self.dropout_d(output, training)\n",
        "        output = self.last_dense(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjr6LdsJmsbj"
      },
      "source": [
        "# print(len(set(train_labels)))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-XyzR5meKz"
      },
      "source": [
        "VOCAB_SIZE = num_words\n",
        "\n",
        "EMB_DIM = glove_dim\n",
        "NB_FILTERS = 256\n",
        "FFN_UNITS = 256\n",
        "NB_CLASSES = len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tim5aZkunDVE"
      },
      "source": [
        "Dcnn = DCNN(\n",
        "          vocab_size=VOCAB_SIZE,\n",
        "          emb_dim=EMB_DIM,\n",
        "          nb_filters=NB_FILTERS,\n",
        "          FFN_units=FFN_UNITS,\n",
        "          nb_classes=NB_CLASSES,\n",
        "          dropout_rate=DROPOUT_RATE,\n",
        "          ngrams=[3,4,5])"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4x7OjpEnQYP"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIINUs5RnofL"
      },
      "source": [
        "checkpoint_path = \"/content/CNN_for_NLP/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored\")"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7FU1n5unr-c",
        "outputId": "265e9b26-69aa-44dd-ec05-9966654828cd"
      },
      "source": [
        "Dcnn.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NB_EPOCHS,\n",
        "    shuffle=True,\n",
        "    validation_data=(test_inputs, test_labels))\n",
        "# ckpt_manager.save()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 28s 280ms/step - loss: 0.6731 - accuracy: 0.6811 - val_loss: 0.3984 - val_accuracy: 0.8268\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 27s 280ms/step - loss: 0.3508 - accuracy: 0.8517 - val_loss: 0.3243 - val_accuracy: 0.8599\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 27s 281ms/step - loss: 0.2814 - accuracy: 0.8847 - val_loss: 0.2797 - val_accuracy: 0.8812\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 28s 281ms/step - loss: 0.2475 - accuracy: 0.8978 - val_loss: 0.2746 - val_accuracy: 0.8846\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 27s 281ms/step - loss: 0.1954 - accuracy: 0.9240 - val_loss: 0.2604 - val_accuracy: 0.8933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe1521e5290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stS2JN_P1y1C"
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARQOZt9QPvLs"
      },
      "source": [
        "Dcnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}