{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Classification-CNN-for-NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdgmP4nsOKhZ0AdbgZF2Rz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smf-9000/Text-Intent-Classification/blob/main/Text_Classification_CNN_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eeafJFrkITP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "start link:\n",
        "https://www.udemy.com/course/modern-nlp\n",
        "\n",
        "[TODO] Try some external word embedding.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC7rHYVrBeyP"
      },
      "source": [
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uko_JHIVDZHK"
      },
      "source": [
        "!mkdir /content/CNN_for_NLP\n",
        "!mkdir /content/CNN_for_NLP/data\n",
        "!mkdir /content/CNN_for_NLP/ckpt\n",
        "!unzip /content/trainingandtestdata.zip -d /content/CNN_for_NLP/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbvHJvHEj4C2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezHOU6nREmhT"
      },
      "source": [
        "cols = [\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlaECbNnE-ih"
      },
      "source": [
        "train_data = pd.read_csv(\n",
        "    \"/content/CNN_for_NLP/data/training.1600000.processed.noemoticon.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\")\n",
        "test_data = pd.read_csv(\n",
        "    \"/content/CNN_for_NLP/data/testdata.manual.2009.06.14.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sEjyB2iM2-n"
      },
      "source": [
        "remove_n = 1300000\n",
        "drop_indices = np.random.choice(train_data.index, remove_n, replace=False)\n",
        "train_data = train_data.drop(drop_indices).reset_index(drop=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac_Mtsp4vCwG"
      },
      "source": [
        "(train_data['sentiment'] == 4).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4_FrhDJFwQi"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GF8Qu37KTAL"
      },
      "source": [
        "train_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMlVPO6cF7y7"
      },
      "source": [
        "train_data.drop(\n",
        "    [\"id\",\"date\",\"query\",\"user\"],\n",
        "    axis =1,\n",
        "    inplace=True)\n",
        "test_data.drop(\n",
        "    [\"id\",\"date\",\"query\",\"user\"],\n",
        "    axis =1,\n",
        "    inplace=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnFbpHkPMAH6"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJqA8T_PQOp2"
      },
      "source": [
        "test_data = test_data[test_data.sentiment.values != 2]  # there are some \"2\" in test set"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftFOn0b5Yy8P"
      },
      "source": [
        "test_data = test_data.reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xQf-YWdOrv3"
      },
      "source": [
        "train_data_labels = train_data.sentiment.values\n",
        "train_data_labels[train_data_labels==4] = 1\n",
        "test_data_labels = test_data.sentiment.values\n",
        "test_data_labels[test_data_labels==4] = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExvjGlGuax-x"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n",
        "  tweet = re.sub(r\"@[A-Za-z0-9]+\",' ', tweet)\n",
        "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\",' ', tweet)\n",
        "  tweet = re.sub(r\"[^a-zA-Z.!?']\",' ', tweet)\n",
        "  tweet = re.sub(r\" +\", \" \", tweet)\n",
        "  return tweet"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoYiwaNWauaR"
      },
      "source": [
        "train_data_clean = [clean_tweet(tweet) for tweet in train_data.text]\n",
        "test_data_clean = [clean_tweet(tweet) for tweet in test_data.text]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwjKhkOPLxj"
      },
      "source": [
        "# set(test_data_labels)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM5j7pEpYxJd"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "                                                      train_data_clean + test_data_clean,\n",
        "                                                      target_vocab_size=500,\n",
        "                                                      max_subword_length=10)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ggk9MZbS8X"
      },
      "source": [
        "train_inputs = [tokenizer.encode(sentence) for sentence in train_data_clean]\n",
        "test_inputs = [tokenizer.encode(sentence) for sentence in test_data_clean]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2VyaXjOfAvE"
      },
      "source": [
        "train_labels = train_data_labels\n",
        "test_labels = test_data_labels"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE-bFUnle4DA"
      },
      "source": [
        "MAX_LEN = max([len(sentence) for sentence in train_inputs + test_inputs])\n",
        "train_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_inputs,\n",
        "    value=0,\n",
        "    padding=\"post\",\n",
        "    maxlen=MAX_LEN)\n",
        "test_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_inputs,\n",
        "    value=0,\n",
        "    padding=\"post\",\n",
        "    maxlen=MAX_LEN)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqDbXtAheifI"
      },
      "source": [
        ""
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OjS9pkhBH1"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            emb_dim=128,\n",
        "            nb_filters=50,\n",
        "            FFN_units=512,\n",
        "            nb_classes=2,\n",
        "            dropout_rate=0.1,\n",
        "            training=False,\n",
        "            name=\"dcnn\",\n",
        "            ngrams=[2,3,4]):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(\n",
        "                            vocab_size,\n",
        "                            emb_dim)\n",
        "        self.conv1_list = []\n",
        "        for n in ngrams:\n",
        "          conv1 = layers.Conv1D(\n",
        "                            filters=nb_filters,\n",
        "                            kernel_size=n,\n",
        "                            padding=\"valid\",\n",
        "                            activation=\"relu\")\n",
        "          self.conv1_list.append(conv1)\n",
        "\n",
        "        self.pool_1 = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(\n",
        "                            units=FFN_units,\n",
        "                            activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=1,\n",
        "                            activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=nb_classes,\n",
        "                            activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_x = []\n",
        "        for i, _ in enumerate(self.conv1_list):\n",
        "          x_t = self.conv1_list[i](x)\n",
        "          x_t = self.pool_1(x_t)\n",
        "          x_x.append(x_t)\n",
        "        \n",
        "        merged = tf.concat(x_x, axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjr6LdsJmsbj"
      },
      "source": [
        "# print(len(set(train_labels)))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-XyzR5meKz"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "EMB_DIM = 100\n",
        "NB_FILTERS = 200\n",
        "FFN_UNITS = 128\n",
        "NB_CLASSES = len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tim5aZkunDVE"
      },
      "source": [
        "Dcnn = DCNN(\n",
        "          vocab_size=VOCAB_SIZE,\n",
        "          emb_dim=EMB_DIM,\n",
        "          nb_filters=NB_FILTERS,\n",
        "          FFN_units=FFN_UNITS,\n",
        "          nb_classes=NB_CLASSES,\n",
        "          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4x7OjpEnQYP"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIINUs5RnofL"
      },
      "source": [
        "checkpoint_path = \"/content/CNN_for_NLP/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7FU1n5unr-c",
        "outputId": "34d66a9e-83e7-4111-94b5-48eca0cb3dcd"
      },
      "source": [
        "Dcnn.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NB_EPOCHS,\n",
        "    shuffle=True,\n",
        "    validation_data=(test_inputs, test_labels))\n",
        "# ckpt_manager.save()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1172/1172 [==============================] - 34s 29ms/step - loss: 0.4929 - accuracy: 0.7578 - val_loss: 0.4817 - val_accuracy: 0.7382\n",
            "Epoch 2/5\n",
            "1172/1172 [==============================] - 33s 28ms/step - loss: 0.4285 - accuracy: 0.8015 - val_loss: 0.4761 - val_accuracy: 0.7716\n",
            "Epoch 3/5\n",
            "1172/1172 [==============================] - 33s 29ms/step - loss: 0.4003 - accuracy: 0.8175 - val_loss: 0.4626 - val_accuracy: 0.7883\n",
            "Epoch 4/5\n",
            "1172/1172 [==============================] - 33s 29ms/step - loss: 0.3774 - accuracy: 0.8304 - val_loss: 0.4681 - val_accuracy: 0.7967\n",
            "Epoch 5/5\n",
            "1172/1172 [==============================] - 34s 29ms/step - loss: 0.3542 - accuracy: 0.8428 - val_loss: 0.4704 - val_accuracy: 0.7799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f16787d41d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stS2JN_P1y1C"
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}