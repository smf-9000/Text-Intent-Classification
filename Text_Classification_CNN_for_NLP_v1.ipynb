{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text-Classification-CNN-for-NLP-v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smf-9000/Text-Intent-Classification/blob/main/Text_Classification_CNN_for_NLP_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eeafJFrkITP"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "start link:\n",
        "https://www.udemy.com/course/modern-nlp\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC7rHYVrBeyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4a5c94-1f6e-4413-c354-ef3785b47062"
      },
      "source": [
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-08 10:44:38--  http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip [following]\n",
            "--2021-07-08 10:44:38--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  31.7MB/s    in 2.4s    \n",
            "\n",
            "2021-07-08 10:44:41 (31.7 MB/s) - ‘trainingandtestdata.zip’ saved [81363704/81363704]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uko_JHIVDZHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cad306-f97c-44d3-e70c-2089b5a9c88d"
      },
      "source": [
        "!mkdir /content/CNN_for_NLP\n",
        "!mkdir /content/CNN_for_NLP/data\n",
        "!mkdir /content/CNN_for_NLP/ckpt\n",
        "!unzip /content/trainingandtestdata.zip -d /content/CNN_for_NLP/data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/trainingandtestdata.zip\n",
            "  inflating: /content/CNN_for_NLP/data/testdata.manual.2009.06.14.csv  \n",
            "  inflating: /content/CNN_for_NLP/data/training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbvHJvHEj4C2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezHOU6nREmhT"
      },
      "source": [
        "cols = [\"sentiment\",\"id\",\"date\",\"query\",\"user\",\"text\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlaECbNnE-ih"
      },
      "source": [
        "data_from_csv = pd.read_csv(\n",
        "    \"/content/CNN_for_NLP/data/training.1600000.processed.noemoticon.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sEjyB2iM2-n"
      },
      "source": [
        "remove_n = 1000000\n",
        "drop_indices = np.random.choice(data_from_csv.index, remove_n, replace=False)\n",
        "data = data_from_csv.drop(drop_indices).reset_index(drop=True)\n",
        "\n",
        "train_indecies = np.random.choice(data.index, 400000, replace=False)\n",
        "train_data = data.iloc[train_indecies].reset_index(drop=True)\n",
        "\n",
        "test_data = data.drop(train_indecies).reset_index(drop=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac_Mtsp4vCwG"
      },
      "source": [
        "(train_data['sentiment'] == 0).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4_FrhDJFwQi"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GF8Qu37KTAL"
      },
      "source": [
        "train_data.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMlVPO6cF7y7"
      },
      "source": [
        "train_data.drop(\n",
        "    [\"id\",\"date\",\"query\",\"user\"],\n",
        "    axis =1,\n",
        "    inplace=True)\n",
        "test_data.drop(\n",
        "    [\"id\",\"date\",\"query\",\"user\"],\n",
        "    axis =1,\n",
        "    inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnFbpHkPMAH6"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xQf-YWdOrv3"
      },
      "source": [
        "train_data_labels = train_data.sentiment.values\n",
        "train_data_labels[train_data_labels==4] = 1\n",
        "test_data_labels = test_data.sentiment.values\n",
        "test_data_labels[test_data_labels==4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExvjGlGuax-x"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "  tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n",
        "  tweet = re.sub(r\"@[A-Za-z0-9]+\",' ', tweet)\n",
        "  tweet = re.sub(r\"https?://[A-Za-z0-9./]+\",' ', tweet)\n",
        "  tweet = re.sub(r\"[^a-zA-Z.!?']\",' ', tweet)\n",
        "  tweet = re.sub(r\" +\", \" \", tweet)\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoYiwaNWauaR"
      },
      "source": [
        "train_data_clean = [clean_tweet(tweet) for tweet in train_data.text]\n",
        "test_data_clean = [clean_tweet(tweet) for tweet in test_data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwjKhkOPLxj"
      },
      "source": [
        "# set(test_data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM5j7pEpYxJd"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "                                                      train_data_clean + test_data_clean,\n",
        "                                                      target_vocab_size=5000,\n",
        "                                                      max_subword_length=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ggk9MZbS8X"
      },
      "source": [
        "train_inputs = [tokenizer.encode(sentence) for sentence in train_data_clean]\n",
        "test_inputs = [tokenizer.encode(sentence) for sentence in test_data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2VyaXjOfAvE"
      },
      "source": [
        "train_labels = train_data_labels\n",
        "test_labels = test_data_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE-bFUnle4DA"
      },
      "source": [
        "MAX_LEN = max([len(sentence) for sentence in train_inputs + test_inputs])\n",
        "train_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    train_inputs,\n",
        "    value=0,\n",
        "    padding=\"post\",\n",
        "    maxlen=MAX_LEN)\n",
        "test_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    test_inputs,\n",
        "    value=0,\n",
        "    padding=\"post\",\n",
        "    maxlen=MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqDbXtAheifI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4OjS9pkhBH1"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size,\n",
        "            emb_dim=128,\n",
        "            nb_filters=50,\n",
        "            FFN_units=512,\n",
        "            nb_classes=2,\n",
        "            dropout_rate=0.1,\n",
        "            training=False,\n",
        "            name=\"dcnn\",\n",
        "            ngrams=[2,3,4]):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(\n",
        "                            vocab_size,\n",
        "                            emb_dim)\n",
        "        self.conv1_list = []\n",
        "        for n in ngrams:\n",
        "          conv1 = layers.Conv1D(\n",
        "                            filters=nb_filters,\n",
        "                            kernel_size=n,\n",
        "                            padding=\"valid\",\n",
        "                            activation=\"relu\")\n",
        "          self.conv1_list.append(conv1)\n",
        "\n",
        "        self.pool_1 = layers.GlobalMaxPool1D()\n",
        "        self.dense_1 = layers.Dense(\n",
        "                            units=FFN_units,\n",
        "                            activation=\"relu\")\n",
        "        self.dropout_e = layers.Dropout(rate=0.4)\n",
        "        self.dropout_d = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=1,\n",
        "                            activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(\n",
        "                            units=nb_classes,\n",
        "                            activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout_e(x, training)\n",
        "        x_x = []\n",
        "        for i, _ in enumerate(self.conv1_list):\n",
        "          x_t = self.conv1_list[i](x)\n",
        "          x_t = self.pool_1(x_t)\n",
        "          x_x.append(x_t)\n",
        "        \n",
        "        merged = tf.concat(x_x, axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout_d(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjr6LdsJmsbj"
      },
      "source": [
        "# print(len(set(train_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN-XyzR5meKz"
      },
      "source": [
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "\n",
        "EMB_DIM = 128\n",
        "NB_FILTERS = 64\n",
        "FFN_UNITS = 64\n",
        "NB_CLASSES = len(set(train_labels))\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tim5aZkunDVE"
      },
      "source": [
        "Dcnn = DCNN(\n",
        "          vocab_size=VOCAB_SIZE,\n",
        "          emb_dim=EMB_DIM,\n",
        "          nb_filters=NB_FILTERS,\n",
        "          FFN_units=FFN_UNITS,\n",
        "          nb_classes=NB_CLASSES,\n",
        "          dropout_rate=DROPOUT_RATE,\n",
        "          ngrams=[2,3,5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4x7OjpEnQYP"
      },
      "source": [
        "if NB_CLASSES == 2:\n",
        "    Dcnn.compile(\n",
        "        loss=\"binary_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"])\n",
        "else:\n",
        "    Dcnn.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIINUs5RnofL"
      },
      "source": [
        "checkpoint_path = \"/content/CNN_for_NLP/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7FU1n5unr-c",
        "outputId": "fba678e4-5cec-472a-8b1e-25493302b2cc"
      },
      "source": [
        "Dcnn.fit(\n",
        "    train_inputs,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NB_EPOCHS,\n",
        "    shuffle=True,\n",
        "    validation_data=(test_inputs, test_labels))\n",
        "# ckpt_manager.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4675 - accuracy: 0.7752 - val_loss: 0.4185 - val_accuracy: 0.8075\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.4107 - accuracy: 0.8124 - val_loss: 0.4107 - val_accuracy: 0.8109\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3868 - accuracy: 0.8262 - val_loss: 0.4046 - val_accuracy: 0.8162\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3698 - accuracy: 0.8349 - val_loss: 0.4048 - val_accuracy: 0.8170\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.3557 - accuracy: 0.8428 - val_loss: 0.4092 - val_accuracy: 0.8178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f71cf524e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stS2JN_P1y1C"
      },
      "source": [
        "results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}